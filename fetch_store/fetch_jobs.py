from scrapers.ifyoucould import fetch_ifyoucould_jobs
from scrapers.unjobs import fetch_unjobs
from scrapers.workable import fetch_workable_jobs
from fetch_store.store_jobs import store_jobs  

# ‚úÖ Fetch Jobs from Scrapers
def fetch_jobs():
    """Fetch job listings from all sources and return as a dictionary."""
    print("\n‚è≥ Running job scrapers...")

    jobs = {
        "ifyoucould": fetch_ifyoucould_jobs(),
        "unjobs": fetch_unjobs(),
        "workable": fetch_workable_jobs(),
    }

    return jobs  # ‚úÖ Keeps jobs in separate lists by source

# ‚úÖ Main Function: Run Scrapers Once (Triggered by GitHub Actions)
def run_scrapers():
    jobs = fetch_jobs()

    # ‚úÖ Send to Firestore via `store_jobs.py`
    if any(jobs.values()):  # ‚úÖ Checks if any list has jobs
        total_jobs = sum(len(v) for v in jobs.values())
        print(f"\nüíæ Sending {total_jobs} jobs to Firestore...")
        store_jobs(jobs)  # ‚úÖ Calls `store_jobs.py`
    else:
        print("\n‚ùå No jobs found across all sources.")

# ‚úÖ Run Scrapers (No Loop, GitHub Actions Handles Scheduling)
if __name__ == "__main__":
    run_scrapers()